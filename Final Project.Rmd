---
title: "Final Project Analysis"
author: "Claire King"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

# Introduction

Agronomists each have their list of important aspects of crop production. This list may include nitrogen, potassium, phosphorus, soil fertility, irrigation, growing degree days, and/or tillage practices. However, one thing does not often make their list, pollinators. Pollinators are vital in agricultural production, yet declining in population numbers.

In this study, we utilize a data set that was collected in Maine, USA to see how certain pollinators can influence the prediction of yields of wild blueberries (<https://www.kaggle.com/datasets/saurabhshahane/wild-blueberry-yield-prediction/data>). If pollinators are seen to significantly improve crop yields predictions, then there might be more economic incentive for major corporations to put major investment into the rehabilitation of their populations.

In this data set, four pollinators are present, honeybees, bumblebees, mining bees (andrena), and mason bees (osmia). Each is described in the following way: honeybees are the most widely known pollinator, bumblebees are known to be very strong pollinators, mining bees are important for early-blooming plants and are ground-nesting, and mason bees are known to be great pollinators of fruit trees and crops. As we dive deeper into this data, it is important to keep these relationships in mind to see if they are reflected in our analysis.

With this data, we hope to evaluate the predictive ability of these pollinators on wild blueberry yields.

# Methods

For this analysis, we want to do linear regression to try and see if predictability is majorly influence by the previously mentioned pollinators and 11 other covariates.

## Data Exploration 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(tseries)
library(car)
library(glmnet)
```

```{r}
## Loading data and cleaning 
# Load in data & summarize 
blueberry_raw <- read.csv("WildBlueberryPollinationSimulationData.csv")
head(blueberry_raw)
summary(blueberry_raw)

# First Column is not needed 
blueberry_data <- blueberry_raw[ ,-1]
head(blueberry_data)
summary(blueberry_data)
str(blueberry_data)

```

As we can see from the summary table above, all covariates are numerical variables. However, it is important to notice that some predictors seem to be related to one another, such as MaxOfUpperTRange and MinOfUpperTRange. This might pose issues in our further analysis.

Now that we have a general understanding of the format of our data, we must explore the distributions and correlations between the predictors themselves.

```{r}
## Exploratory Data Analysis 

# Visualize each predictor to check for outliers through using histograms 
for (colname in names(blueberry_data)) {
  if (is.numeric(blueberry_data[[colname]])) {
    p <- ggplot(blueberry_data, aes_string(x = colname)) +
      geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black") +
      ggtitle(paste("Histogram of", colname))
    print(p)
  }
} # -> edit these to match the needs of each histogram 

ggplot(blueberry_data, aes(x = andrena)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = osmia)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = AverageRainingDays)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = fruitset)) +
  geom_histogram(binwidth = 0.01, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = fruitmass)) +
  geom_histogram(binwidth = 0.01, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")


ggplot(blueberry_data, aes(x = MaxOfUpperTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = MaxOfLowerTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = MinOfUpperTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = MinOfLowerTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = AverageOfUpperTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = AverageOfLowerTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")

ggplot(blueberry_data, aes(x = seeds)) +
  geom_histogram(binwidth = 2, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")


# Assessing variables against one another 

pairs(blueberry_data[, 1:8])
pairs(blueberry_data[, 9:16]) 


```

To begin, we would like to visualize the distributions through histograms. However, as seen through the first for loop, we need variable bin sizes to accomplish this. Therefore, the next step of ggplot strings edits the bin size of each predictor to fit its need. From these histograms, we are mostly satisfied with the distributions that are present! We must note the presence of a few outliers in fruitset and fruitmass.

Next, we plotted all covariates against one another using the pairs() function. Most of the graphs are uninformative, besides those that are associated with fruitset, fruitmass, and seeds, which all seem to have a very strong positive correlation.

## Model Creation 

Now that we have more understanding of our data, we can begin to fit our linear model.

```{r}
## Creating the linear model 

original_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + AverageOfUpperTRange + MaxOfLowerTRange + MinOfLowerTRange + AverageOfLowerTRange + RainingDays + AverageRainingDays + fruitset + fruitmass + seeds, data = blueberry_data)
summary(original_mod)

```

From our first linear model, as predicted, we see that two covariates were not included due to perfect colinearity. This is concerning. Therefore, the first assumption that we wanted to test was multicolinearity.

## Testing Assumptions 

```{r}
### Regression Assumptions Verification 

## Multicollinearity assessment -> model creation has already taken out two variables due to almost perfect collinearity 

sec_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + AverageOfUpperTRange + MaxOfLowerTRange +   RainingDays + AverageRainingDays + fruitset + fruitmass + seeds, data = blueberry_data)
summary(sec_mod)

alias(sec_mod)
vif(sec_mod)

```

We can see that there are linearly dependent covariates in the original model. Therefore, we will take only keep the AverageOfUpperTRange and AverageOfLowerTRange to keep the most information without observing colinearity.

```{r}

# Create new model 
avg_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageOfLowerTRange +
                    RainingDays + AverageRainingDays +
                    fruitset + fruitmass + seeds,
                  data = blueberry_data)


# Test collinearity again 

vif(avg_mod) # Still having collinearity! 

alias(avg_mod)

cor(blueberry_data[, c("AverageOfUpperTRange", "AverageOfLowerTRange", 
                  "RainingDays", "AverageRainingDays", 
                  "fruitset", "fruitmass", "seeds")]) # identify correlations and clean model further 

final_data <- blueberry_data[, c("clonesize", "honeybee", "bumbles", "andrena" ,"osmia",
                                 "AverageOfUpperTRange", "AverageRainingDays", 
                                 "fruitmass", "yield")]

final_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageRainingDays + fruitmass,
                  data = blueberry_data)
vif(final_mod) # -> yay! these all look great :) 

```

From the code chunk above, one can see that it took more refining to get to a model that did not violate the multicolinearity assumption. Additionally, keeping the average values, rather than the maximums and minimums, gives us the most information possible. Now, we can move on to checking the rest of the assumptions of linear regression.

```{r}

### Continue on with other assumptions 

## Linearity 

plot(final_mod$fitted.values, resid(final_mod),
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

## Normality of Residuals 

qqnorm(resid(final_mod))
qqline(resid(final_mod), col = "red")

hist(resid(final_mod), main = "Histogram of Residuals", xlab = "Residuals")

## Homoscedasticity (constant variance of residuals)

plot(final_mod$fitted.values, rstudent(final_mod),
     xlab = "Fitted values", ylab = "Studentized Residuals",
     main = "Check for Heteroskedasticity")
abline(h = 0, col = "red") # -> looks ok! 

bptest <- (final_mod$res^2) / mean(final_mod$res^2)
par(mfrow = c(1,2))
plot(blueberry_data$yield, bptest, ylab = "Standardized Sq Res", xlab = "Covariate")
abline(a = lm(bptest~blueberry_data$yield)$coef[1], b = lm(bptest~blueberry_data$yield)$coef[2], col = "red") # -> Looks ok-ish

bptest(final_mod) # -> look for explanation on why this may not matter, if you wanted assumption to be absolutely satified due to wanting inference, you would need to do .... 

# Heteroscedasticity observed ... Oh No! -> Correct using sandwich estimators 
coeftest(final_mod, vcov = vcovHC(final_mod, type = "HC1"))
summary(final_mod) 

## Independence of Observations 
# explain about time dependence/clustering 


```

Beginning with linearity, I do not see the presence of a fan shape, or any other shape, which leads me to say that this assumption is upheld

For the normality assumption, the qqplot has some slight deviation in the line, but nothing concerning. The histogram also moves me to say that the normality assumption is upheld, due to its normal distribution.

When checking the homoscedasticity assumption, there does not seem to be a lot of reason for concern from the plots. The residuals vs. fitted values plot does not have any significant shape, while the standardized squared residuals vs. the covariates only has one outlier present. However, when a Breusch-Pagan test is run, there is evidence of heteroscedasticity in the data. This is concerning, so we corrected using sandwich estimators. When using sandwich estimators, we see that standard errors are generally larger, which is expected. It is important to note how the standard errors of honeybee significantly increased, show its variability.

Even with the sandwich estimators, the Breusch-Pagan test still showed presence of heteroscedasticity. For our purposes of prediction, addressing this further is not completely necessary as we are mainly concerned with minimizing error on new data. Meanwhile, with inference, heteroscedasticity can inflate type 1 errors, cause your confidence intervals to be unreliable, and have biased standard errors. These are not major concerns in prediction, therefore we can continue on.

## Variable Selection 

```{r}

### Variable Selection 

## Try first with forward selection 

intOnly <- lm(yield ~ 1, data = final_data)

mod <- lm(yield ~ ., data = final_data)
out_forward_bic <- step(object = intOnly, direction = "forward",
scope = formula(mod), trace = T, k = log(nrow(final_data)))
summary(out_forward_bic) # -> lowest AIC of 8140.98, kept all covariates! 

## Try now with backward selection 
out_backward_bic <- step(object = mod, direction = "backward",
scope = formula(mod), trace = T, k = log(nrow(final_data)))
summary(out_backward_bic) # -> lowest AIC of 8141.0, kept all covariates! 

## Now with lasso 

x <- model.matrix(~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageRainingDays + fruitmass,
                  data = blueberry_data)[, -1]

lasso_fit <- glmnet(x = x, y = final_data$yield, alpha = 1, nlambda = 100)
coef(lasso_fit) # -> maybe out take out bumbles and andrena, but i will keep them in 


```

Both backward and forward selection showed that the lowest AIC value kept all covariates in the model. When using lasso, it became known that we could take out bumbles and/or andrena, but we decided to keep them in due to the previous results.

## Hypothesis Testing 

```{r}
### Hypothesis Testing 

## Run t-tests on selected model 

final_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageRainingDays + fruitmass,
                  data = blueberry_data)
summary(final_mod) #-> all are statistically significant 
summary(final_mod)$r.squared# -> fantastic!
summary(final_mod)$adj.r.squared# -> fantastic! 

rmse <- sqrt(mean(residuals(final_mod)^2))
print(rmse)  # -> within acceptable range! 

```

From our final model, we can see that each covariate is statistically significant. This is further validated through the high values of R^2^ and adjusted R\^2. Finally, we calculated the RMSE to be 181.3048, which is in the acceptable range!

## Cross-Validation  

```{r}
### Cross-Validation 

set.seed(1)

n <- dim(final_data)[1]
m <- floor(n * 0.8)

train_idx <- sample(1:n, m, replace = FALSE)

train_set <- final_data[train_idx,]
test_set <- final_data[-train_idx,]
y_true <- test_set$yield

cross_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageRainingDays + fruitmass, data = train_set)
summary(cross_mod)

y_hat_1 <- predict(cross_mod, test_set)
pred_error_1 <- mean((y_hat_1 - y_true)^2)
sqrt(pred_error_1) # -> Models predictions are off by 173.7 units, which is close to the training RMSE of 185.4 -> very close with no overfitting!! 

```

To finalize our model, we use cross-validation to test the predictability of our model. To do this, we kept 80% of the data for training and the following 20% for testing. In the training set, the RMSE was 185.4 and, in the testing set, the RMSE value was 173.7344. These two values are very close to one another, which shows the high predictability of our model, along with the low presence of overfitting.

# Analysis

```{r}
summary(final_mod)
```

With our final model selected, the impact of each selected covariate can be assessed in the following lines. It is important to note that each of these analysis are assuming that all other covariates are being held constant.

For a one unit increase in clonesize, yield decreases by 34 units.

For a one unit increase in honeybee visits, yield increases by 168 units.

For a one unit increase in bumblebee visits, yield increases by 1330 units.

For a one unit increase in mining bee visits, yield increases by 401 units.

For a one unit increase in mason bee visits, yield increases by 933 units.

For a one unit increase in the average upper temperature range, yield decreases by 38 units.

For a one unit increase in the average raining days, yield decreases by 1859 units.

For a one unit increase in fruit mass, yield increases by 23,790 units.

```{r}
confint(final_mod)
```

From the confidence intervals above, it is important to notice how wide the bumblebee interval is. This makes sense as the variability was shown through the increase of standard errors when we tested for homoscedasticity. Other than that, these intervals look fantastic!

# Results

The analysis above gives important insight into the predictive ability of pollinators when predicting yield. The predictive ability of each pollinator was statistically significant for predicting yield. More specifically, the highest influences of the pollinators came from the bumblebees and mason bees.

However, the pollinators were not the most impactful covariates. Average raining days and fruit mass had the most impact on yield. Meanwhile, clone size and average upper temperature range had modest but statistically significant impacts on yield prediction as well.

# Discussion

The results show that pollinators are important factors when trying to predict yield of wild blueberries, with the most impactful being the bumblebees and mason bees. This makes sense, as bumblebees are strong pollinators in general, and mason bees are strong pollinators for fruit trees, such as blueberries!

Additionally, the two covariates with the largest effect are average raining days and fruit mass. Fruit mass is expected. However, the negative impact of average raining days leads me to assume that either over-watering is very harmful in blueberries or that the plots they were grown in are prone to flooding.

# Conclusion

All in all, pollinators are significant contributors to predicting yield. This is great news for pollinators as large seed companies are always trying to increase their predictability. Hopefully, more money can be put into rehabilitating their populations, due to this conclusion.

In the future, it would be informative to gather more data on different pollinators, such as wasps, to better understand if this predictability is limited to bees or not. Also, it would be interesting to try this study in different crops to see how this outcome compares to others.

# References 
