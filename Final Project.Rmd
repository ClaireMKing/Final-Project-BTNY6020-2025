---
title: "Final Project Analysis"
author: "Claire King"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

# Introduction

Agronomists each have their list of important aspects of crop production. This list may include nitrogen, potassium, phosphorus, soil fertility, irrigation, growing degree days, and/or tillage practices. However, one thing does not often make their list, pollinators. Pollinators are vital in agricultural production, yet declining in population numbers.

In this study, we utilize a data set that was collected in Maine, USA to see how certain pollinators can influence the prediction of yields of wild blueberries (<https://www.kaggle.com/datasets/saurabhshahane/wild-blueberry-yield-prediction/data>). If pollinators are seen to significantly improve crop yields predictions, then there might be more economic incentive for major corporations to put major investment into the rehabilitation of their populations.

In this data set, four pollinators are present, honeybees, bumblebees, mining bees (andrena), and mason bees (osmia). Each is described in the following way: honeybees are the most widely known pollinator, bumblebees are known to be very strong pollinators, mining bees are important for early-blooming plants and are ground-nesting, and mason bees are known to be great pollinators of fruit trees and crops. As we dive deeper into this data, it is important to keep these relationships in mind to see if they are reflected in our analysis.

With this data, we hope to evaluate the predictive ability of these pollinators on wild blueberry yields.

# Methods

For this analysis, we want to do a linear regression to  see if predictability is majorly influenced by the previously mentioned pollinators and 11 other covariates.

## Data Exploration 

We will first load in the requirements to run this analysis. All packages are able to be run on the most updated version of RStudio (v 2025.05.0-496). 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(tseries)
library(car)
library(glmnet)
```

Next, the data must be loaded in, then exploratory data analysis will be conducted through the use of head(), summary(), and str(). The head function gives a quick view into the rows and columns that are in the data frame. When this is run on the raw data, it comes to attention that the first column is not needed for analysis. 
Through the use of the summary and str functions, we are able to see that all of the columns are numeric, which makes for easier analysis. Additionally, we are able to view the spread of the data to get a preview into the variation present and where the means are. 

```{r}
## Loading data and cleaning 
# Load in data & summarize 
blueberry_raw <- read.csv("WildBlueberryPollinationSimulationData.csv")
head(blueberry_raw)
summary(blueberry_raw)

# First Column is not needed 
blueberry_data <- blueberry_raw[ ,-1]
head(blueberry_data)
summary(blueberry_data)
str(blueberry_data)

```

It is important to notice that some predictors seem to be related to one another, such as MaxOfUpperTRange and MinOfUpperTRange. This might pose issues in our further analysis.

Now that we have a general understanding of the format of our data, we must explore the distributions and correlations between the predictors themselves.

```{r}
## Exploratory Data Analysis 

# Visualize each predictor to check for outliers through using histograms 
for (colname in names(blueberry_data)) {
  if (is.numeric(blueberry_data[[colname]])) {
    p <- ggplot(blueberry_data, aes_string(x = colname)) +
      geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black") +
      ggtitle(paste("Histogram of", colname))
    print(p)
  }
} # -> edit these to match the needs of each histogram 

ggplot(blueberry_data, aes(x = andrena)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = osmia)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = AverageRainingDays)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = fruitset)) +
  geom_histogram(binwidth = 0.01, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = fruitmass)) +
  geom_histogram(binwidth = 0.01, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")


ggplot(blueberry_data, aes(x = MaxOfUpperTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = MaxOfLowerTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = MinOfUpperTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = MinOfLowerTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = AverageOfUpperTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")
ggplot(blueberry_data, aes(x = AverageOfLowerTRange)) +
  geom_histogram(binwidth = 7, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")

ggplot(blueberry_data, aes(x = seeds)) +
  geom_histogram(binwidth = 2, fill = "steelblue", color = "black") +
  ggtitle("Histogram of Your Variable")


# Assessing variables against one another 

pairs(blueberry_data[, 1:8])
pairs(blueberry_data[, 9:16]) 


```

To begin, we would like to visualize the distributions through histograms (geom_histogram()). However, as seen through the first for loop, we need variable bin sizes to accomplish this. Therefore, the next step of ggplot strings edits the bin size of each predictor to fit its need. From these histograms, we are mostly satisfied with the distributions that are present! We must note the presence of a few outliers in fruitset and fruitmass.

Next, we plotted all covariates against one another using the pairs() function. Most of the graphs are not very informative, besides those that are associated with fruitset, fruitmass, and seeds, which all seem to have a very strong positive correlation.

From this exploratory data analysis, we now understand how the data is distributed, correlations between the covariates, and some aspects of our data to look out for. 

## Model Creation 

Now that we have more understanding of our data, we can begin to fit our linear model using the lm() function.

```{r}
## Creating the linear model 

original_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia + 
                     MaxOfUpperTRange + MinOfUpperTRange + AverageOfUpperTRange + 
                     MaxOfLowerTRange + MinOfLowerTRange + AverageOfLowerTRange + RainingDays + 
                     AverageRainingDays + fruitset + fruitmass + seeds, data = blueberry_data)
summary(original_mod)

```

From our first linear model, as predicted, we see that two covariates were not included due to perfect colinearity. This is concerning. Therefore, the first assumption that we wanted to test was multicolinearity. To do this, we will use the alias() and vif() functions to help assess multicollinearity. 

## Testing Assumptions 

```{r}
### Regression Assumptions Verification 

## Multicollinearity assessment -> model creation has already taken out two variables due to almost perfect collinearity 

sec_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia + 
                MaxOfUpperTRange + MinOfUpperTRange + AverageOfUpperTRange + MaxOfLowerTRange +   
                RainingDays + AverageRainingDays + fruitset + fruitmass + seeds, data = blueberry_data)
summary(sec_mod)

alias(sec_mod)
vif(sec_mod)

```

We can see that there are linearly dependent covariates in the original model, this is from the numbers from the vif output being large. The ideal value is 1, with 1-5 being somewhat acceptable. To combat this issue , we will take only keep the AverageOfUpperTRange and AverageOfLowerTRange to keep the most information without observing collinearity.

```{r}

# Create new model 
avg_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageOfLowerTRange +
                    RainingDays + AverageRainingDays +
                    fruitset + fruitmass + seeds,
                  data = blueberry_data)


# Test collinearity again 

vif(avg_mod) # Still having collinearity! 

alias(avg_mod)

cor(blueberry_data[, c("AverageOfUpperTRange", "AverageOfLowerTRange", 
                  "RainingDays", "AverageRainingDays", 
                  "fruitset", "fruitmass", "seeds")]) # identify correlations and clean model further 

final_data <- blueberry_data[, c("clonesize", "honeybee", "bumbles", "andrena" ,"osmia",
                                 "AverageOfUpperTRange", "AverageRainingDays", 
                                 "fruitmass", "yield")]

final_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageRainingDays + fruitmass,
                  data = blueberry_data)
vif(final_mod) # -> yay! these all look great :) 

```

From the code chunk above, one can see that it took more refining to get to a model that did not violate the multicolinearity assumption. This can be seen through the vif values ranging from 1-3, which is suitable for this analysis. Additionally, keeping the average values, rather than the maximums and minimums, gives us the most information possible. Now, we can move on to checking the rest of the assumptions of linear regression.

```{r}

### Continue on with other assumptions 

## Linearity 

plot(final_mod$fitted.values, resid(final_mod),
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

## Normality of Residuals 

qqnorm(resid(final_mod))
qqline(resid(final_mod), col = "red")

hist(resid(final_mod), main = "Histogram of Residuals", xlab = "Residuals")

## Homoscedasticity (constant variance of residuals)

plot(final_mod$fitted.values, rstudent(final_mod),
     xlab = "Fitted values", ylab = "Studentized Residuals",
     main = "Check for Heteroskedasticity")
abline(h = 0, col = "red") # -> looks ok! 

bptest <- (final_mod$res^2) / mean(final_mod$res^2)
par(mfrow = c(1,2))
plot(blueberry_data$yield, bptest, ylab = "Standardized Sq Res", xlab = "Covariate")
abline(a = lm(bptest~blueberry_data$yield)$coef[1], b = lm(bptest~blueberry_data$yield)$coef[2], col = "red") # -> Looks ok-ish

bptest(final_mod) # -> look for explanation on why this may not matter, if you wanted assumption to be absolutely satified due to wanting inference, you would need to do .... 

# Heteroscedasticity observed ... Oh No! -> Correct using sandwich estimators 
coeftest(final_mod, vcov = vcovHC(final_mod, type = "HC1"))
summary(final_mod) 

## Independence of Observations 
# explain about time dependence/clustering 


```

Beginning with linearity, I do not see the presence of a fan shape, or any other shape, in the residuals vs. fitted values plot, which leads me to say that this assumption is upheld

For the normality assumption, the qqplot has some slight deviation in the line, but nothing concerning. The histogram also moves me to say that the normality assumption is upheld, due to its normal distribution.

When checking the homoscedasticity assumption, there does not seem to be a lot of reason for concern from the plots. The residuals vs. fitted values plot does not have any significant shape, while the standardized squared residuals vs. the covariates only has one outlier present. However, when a Breusch-Pagan test is run, there is evidence of heteroscedasticity in the data. This is concerning, so we corrected using sandwich estimators. When using sandwich estimators, we see that standard errors are generally larger, which is expected. It is important to note how the standard errors of honeybee significantly increased, show its variability.

Even with the sandwich estimators, the Breusch-Pagan test still showed presence of heteroscedasticity. For our purposes of prediction, addressing this further is not completely necessary as we are mainly concerned with minimizing error on new data. Meanwhile, with inference, heteroscedasticity can inflate type 1 errors, cause your confidence intervals to be unreliable, and have biased standard errors. These are not major concerns in prediction, therefore we can continue on.

The final assumption to consider is the independence of observations. This assumption can be challenging to assess directly; therefore, examining the covariates provides a practical approach. Multicollinearity has already been addressed by removing variables that were highly correlated, such as the minimum and maximum temperature values and variables related to rainy days. These covariates were either time-dependent or redundant. Additionally, because all observations are from the same year and there are no repeated measurements, the data can be reasonably assumed to meet the independence assumption.

Now that all the assumptions have been checked, we can move on to variable selection. 

## Variable Selection 

```{r}

### Variable Selection 

## Try first with forward selection 

intOnly <- lm(yield ~ 1, data = final_data)

mod <- lm(yield ~ ., data = final_data)
out_forward_bic <- step(object = intOnly, direction = "forward",
scope = formula(mod), trace = T, k = log(nrow(final_data)))
summary(out_forward_bic) # -> lowest AIC of 8140.98, kept all covariates! 

## Try now with backward selection 
out_backward_bic <- step(object = mod, direction = "backward",
scope = formula(mod), trace = T, k = log(nrow(final_data)))
summary(out_backward_bic) # -> lowest AIC of 8141.0, kept all covariates! 

# ## Now with lasso -> not necessary, but still did! 
# 
# x <- model.matrix(~ clonesize + honeybee + bumbles + andrena + osmia +
#                     AverageOfUpperTRange + AverageRainingDays + fruitmass,
#                   data = blueberry_data)[, -1]
# 
# lasso_fit <- glmnet(x = x, y = final_data$yield, alpha = 1, nlambda = 100)
# coef(lasso_fit) # -> maybe out take out bumbles and andrena, but i will keep them in 


```

Utilizing the step() function allows us to add or subtract covariates from the model, making it very useful when testing between forward and backward selection. 
Both backward and forward selection showed that the lowest AIC value kept all covariates in the model, with the lowest AICs being ~8140. Therefore, we will keep all covariates in the model! 

## Hypothesis Testing 

```{r}
### Hypothesis Testing 

## Run t-tests on selected model 

final_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageRainingDays + fruitmass,
                  data = blueberry_data)
summary(final_mod) #-> all are statistically significant 
summary(final_mod)$r.squared# -> fantastic!
summary(final_mod)$adj.r.squared# -> fantastic! 

```

We then ran our final model through hypothesis testing with t-tests. From the summary table above, we can see that each covariate is statistically significant. This is further validated through the high values of R^2 and adjusted R\^2, which both have high values ~98%. 

## Cross-Validation  

```{r}
### Cross-Validation 

set.seed(1)

n <- dim(final_data)[1]
m <- floor(n * 0.8)

train_idx <- sample(1:n, m, replace = FALSE)

train_set <- final_data[train_idx,]
test_set <- final_data[-train_idx,]
y_true <- test_set$yield

cross_mod <- lm(yield ~ clonesize + honeybee + bumbles + andrena + osmia +
                    AverageOfUpperTRange + AverageRainingDays + fruitmass, data = train_set)
summary(cross_mod)

y_hat_1 <- predict(cross_mod, test_set)
pred_error_1 <- mean((y_hat_1 - y_true)^2)
sqrt(pred_error_1) # -> Models predictions are off by 173.7 units, which is close to the training RMSE of 185.4 -> very close with no overfitting!! 

```

To finalize our model, we use cross-validation to test the predictability of our model. To do this, we kept 80% of the data for training and the following 20% for testing. In the training set, the RMSE was 185.4 and, in the testing set, the RMSE value was 173.7344. These two values are very close to one another, which shows the high predictability of our model, along with the low presence of overfitting.

# Analysis

```{r}
summary(final_mod)
```

With our final model selected, the impact of each selected covariate can be assessed in the following lines. It is important to note that each of these analysis are assuming that all other covariates are being held constant.

For a one unit increase in clonesize, yield decreases by 34 units.

For a one unit increase in honeybee visits, yield increases by 168 units.

For a one unit increase in bumblebee visits, yield increases by 1330 units.

For a one unit increase in mining bee visits, yield increases by 401 units.

For a one unit increase in mason bee visits, yield increases by 933 units.

For a one unit increase in the average upper temperature range, yield decreases by 38 units.

For a one unit increase in the average raining days, yield decreases by 1859 units.

For a one unit increase in fruit mass, yield increases by 23,790 units.

```{r}
confint(final_mod)
```

From the confidence intervals above, it is important to notice how wide the bumblebee interval is. This makes sense as the variability was shown through the increase of standard errors when we tested for homoscedasticity. Other than that, these intervals look fantastic!

# Results

The analysis above gives important insight into the predictive ability of pollinators when predicting yield. The predictive ability of each pollinator was statistically significant for predicting yield. More specifically, the highest influences of the pollinators came from the bumblebees and mason bees.

However, the pollinators were not the most impactful covariates. Average raining days and fruit mass had the most impact on yield. Meanwhile, clone size and average upper temperature range had modest but statistically significant impacts on yield prediction as well.

# Discussion

The results show that pollinators are important factors when trying to predict yield of wild blueberries, with the most impactful being the bumblebees and mason bees. This makes sense, as bumblebees are strong pollinators in general, and mason bees are strong pollinators for fruit trees, such as blueberries!

Additionally, the two covariates with the largest effect are average raining days and fruit mass. Fruit mass is expected. However, the negative impact of average raining days leads me to assume that either over-watering is very harmful in blueberries or that the plots they were grown in are prone to flooding.

# Conclusion

All in all, pollinators are significant contributors to predicting yield. This is great news for pollinators as large seed companies are always trying to increase their predictability. Hopefully, more money can be put into rehabilitating their populations, due to this conclusion.

In the future, it would be informative to gather more data on different pollinators, such as wasps, to better understand if this predictability is limited to bees or not. Also, it would be interesting to try this study in different crops to see how this outcome compares to others.

# References 

Bettache, Nayel. "Github Tutorial" BTRY 6020, May 05, Cornell University. Tutorial.

Bettache, Nayel. "Lectures 1 - 26, Labs 1 - 12" BTRY 6020, January 27 - May 05, Cornell University. Class Lecture.

Shahane, Saurabh. “Wild Blueberry Yield Prediction.” Kaggle, 26 Feb. 2021, www.kaggle.com/datasets/saurabhshahane/wild-blueberry-yield-prediction?resource=download. 

